{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e4c9630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pintusingh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pintusingh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/pintusingh/Desktop/fake-job-detection/venv/lib/python3.11/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <B9202094-7D52-318C-99CF-7034B0E9F28D> /Users/pintusingh/Desktop/fake-job-detection/venv/lib/python3.11/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXGBoostError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstem\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/fake-job-detection/venv/lib/python3.11/site-packages/xgboost/__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracker  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective, dask\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     Booster,\n\u001b[32m     10\u001b[39m     DataIter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     build_info,\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/fake-job-detection/venv/lib/python3.11/site-packages/xgboost/tracker.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntEnum, unique\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LIB, _check_call, make_jcargs\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_family\u001b[39m(addr: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m     13\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get network family from address.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/fake-job-detection/venv/lib/python3.11/site-packages/xgboost/core.py:269\u001b[39m\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[32m    268\u001b[39m \u001b[38;5;66;03m# load the XGBoost library globally\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m _LIB = \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_call\u001b[39m(ret: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    273\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[32m    274\u001b[39m \n\u001b[32m    275\u001b[39m \u001b[33;03m    This function will raise exception when error occurs.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    281\u001b[39m \u001b[33;03m        return value from API calls\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/fake-job-detection/venv/lib/python3.11/site-packages/xgboost/core.py:222\u001b[39m, in \u001b[36m_load_lib\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_success:\n\u001b[32m    221\u001b[39m         libname = os.path.basename(lib_paths[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(\n\u001b[32m    223\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[33mXGBoost Library (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) could not be loaded.\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[33mLikely causes:\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[33m  * OpenMP runtime is not installed\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[33m    - vcomp140.dll or libgomp-1.dll for Windows\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[33m    - libomp.dylib for Mac OSX\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[33m    - libgomp.so for Linux and other UNIX-like OSes\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[33m    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\u001b[39m\n\u001b[32m    231\u001b[39m \n\u001b[32m    232\u001b[39m \u001b[33m  * You are running 32-bit Python on a 64-bit OS\u001b[39m\n\u001b[32m    233\u001b[39m \n\u001b[32m    234\u001b[39m \u001b[33mError message(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_error_list\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    236\u001b[39m         )\n\u001b[32m    237\u001b[39m     _register_log_callback(lib)\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(ver: \u001b[38;5;28mstr\u001b[39m) -> Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n",
      "\u001b[31mXGBoostError\u001b[39m: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/pintusingh/Desktop/fake-job-detection/venv/lib/python3.11/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <B9202094-7D52-318C-99CF-7034B0E9F28D> /Users/pintusingh/Desktop/fake-job-detection/venv/lib/python3.11/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libomp.dylib' (no such file)\"]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.sparse import hstack\n",
    "import collections\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51be95a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95ebbe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ace6e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/fake_job_postings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ceaa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = data[\"location\"].str.split(\",\", expand= True, n= 2)\n",
    "location.columns = [\"country\", \"state\", \"city\"]\n",
    "data[[\"country\", \"state\", \"city\"]] = location\n",
    "data = data.drop(columns= \"location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56e42364",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary = data[\"salary_range\"].str.split(\"-\", expand= True, n= 1)\n",
    "data[[\"min_salary\", \"max_salary\"]] = salary\n",
    "data = data.drop(columns= \"salary_range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a53e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(\"N/A\")\n",
    "data[\"state\"] = data[\"state\"].str.strip().apply(lambda x: \"N/A\" if x == '' else x)\n",
    "data[\"country\"] = data[\"country\"].str.strip().apply(lambda x: \"N/A\" if x == '' else x)\n",
    "data[\"city\"] = data[\"city\"].str.strip().apply(lambda x: \"N/A\" if x == '' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1077b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [\"company_profile\", \"description\", \"requirements\", \"benefits\"]:\n",
    "    data[i] = data[i].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0019930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job_id                 0\n",
       "title                  0\n",
       "department             0\n",
       "company_profile        0\n",
       "description            0\n",
       "requirements           0\n",
       "benefits               0\n",
       "telecommuting          0\n",
       "has_company_logo       0\n",
       "has_questions          0\n",
       "employment_type        0\n",
       "required_experience    0\n",
       "required_education     0\n",
       "industry               0\n",
       "function               0\n",
       "fraudulent             0\n",
       "country                0\n",
       "state                  0\n",
       "city                   0\n",
       "min_salary             0\n",
       "max_salary             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630364d3",
   "metadata": {},
   "source": [
    "Prototype Selection with K-Means + TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32293f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer define\n",
    "ps = PorterStemmer()\n",
    "stop = set(stopwords.words('english'))\n",
    "def tokenizer (doc):\n",
    "    sentences = sent_tokenize(doc)\n",
    "    tokens = []\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        words = [ps.stem(word) for word in words]\n",
    "        tokens+=words\n",
    "    return [w.lower() for w in tokens if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d4c97f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pintusingh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pintusingh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import collections\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_selection import RFE\n",
    "from matplotlib import pyplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f041d077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (17880, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combine text features\n",
    "df1 = data.copy()\n",
    "text_feature = df1[['title', 'department','company_profile','description','requirements','benefits']].apply(lambda x: ' '.join(x), axis = 1)\n",
    "\n",
    "# TF-IDF without custom tokenizer (much faster)\n",
    "tfidf = TfidfVectorizer(\n",
    "    stop_words='english',   # automatically removes stopwords\n",
    "    max_features=5000,      # optional: limit number of features to speed up\n",
    "    lowercase=True,\n",
    "    use_idf=True,\n",
    "    norm='l2',\n",
    "    smooth_idf=True\n",
    ")\n",
    "\n",
    "text_feature = tfidf.fit_transform(text_feature)\n",
    "print(\"TF-IDF shape:\", text_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee6df851",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelEncoder()\n",
    "\n",
    "for col in ['employment_type', 'required_experience', 'required_education', 'industry', 'function', 'country',\n",
    "       'state', 'city']:\n",
    "    df1[col] = lb.fit_transform(df1[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c1e0e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale\n",
    "label_feature = df1[['employment_type', 'required_experience', 'required_education', 'industry', 'function', 'country',\n",
    "       'state', 'city']]\n",
    "scaler = StandardScaler().fit(label_feature)\n",
    "\n",
    "label_feature = scaler.transform(label_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acf6c967",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hstack((text_feature, label_feature))\n",
    "y = df1['fraudulent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50b4b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.10, random_state= 42, stratify= y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size= 0.11, random_state= 42, stratify= y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4228370",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmx = X_train[np.array(y_train == 0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2abd2b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fraudulent\n",
       "0    13628\n",
       "1      693\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "567d83c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use K-means to select 693 non-fraudulent cases\n",
    "km = KMeans(n_clusters= 693, random_state= 42).fit(kmx)\n",
    "non_fraud_centers = km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f554919",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ps = vstack([sparse.csr_matrix(non_fraud_centers), X_train[np.array(y_train == 1), :]])\n",
    "y_train_ps = np.concatenate([np.repeat(0, 693), np.repeat(1, 693)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a8a982b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.981054  0.983383  0.982217      1685\n",
      "           1   0.658537  0.627907  0.642857        86\n",
      "\n",
      "    accuracy                       0.966121      1771\n",
      "   macro avg   0.819795  0.805645  0.812537      1771\n",
      "weighted avg   0.965392  0.966121  0.965738      1771\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "knn = KNeighborsClassifier(n_neighbors= 1)\n",
    "knn.fit(X_train_ps, y_train_ps)\n",
    "y_val_pred = knn.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred, digits= 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c13e267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8056448830308467"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_val, y_val_pred)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f021cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_2 = knn.predict(X_test)\n",
    "print(classification_report(y_test, y_test_pred_2, digits= 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23da2e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_test_pred_2)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef08af",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2 = KNeighborsClassifier(n_neighbors= 1)\n",
    "knn2.fit(X_train, y_train)\n",
    "y_val_pred_2 = knn2.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred_2, digits= 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba74fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_val, y_val_pred_2)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_test_2 = knn2.predict(X_test)\n",
    "print(classification_report(y_test, y_val_test_2, digits= 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56263ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_val_test_2)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d859da",
   "metadata": {},
   "source": [
    "Prototype Selection with K-Means + Countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a4df17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pintusingh/Desktop/fake-job-detection/venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# combine text features and vectorize\n",
    "df2 = data.copy()\n",
    "text_feature = df2[['title', 'department','company_profile','description','requirements','benefits']].apply(lambda x: ' '.join(x), axis = 1)\n",
    "\n",
    "bow = CountVectorizer(tokenizer=tokenizer)\n",
    "\n",
    "text_feature = bow.fit_transform(text_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4806f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode label features\n",
    "lb = LabelEncoder()\n",
    "\n",
    "for col in ['employment_type', 'required_experience', 'required_education', 'industry', 'function', 'country',\n",
    "       'state', 'city']:\n",
    "    df2[col] = lb.fit_transform(df2[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1b9d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale\n",
    "label_feature = df2[['employment_type', 'required_experience', 'required_education', 'industry', 'function', 'country',\n",
    "       'state', 'city']]\n",
    "scaler = StandardScaler().fit(label_feature)\n",
    "\n",
    "label_feature = scaler.transform(label_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94b849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hstack((text_feature, label_feature))\n",
    "y = df1['fraudulent']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.10, random_state= 42, stratify= y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size= 0.11, random_state= 42, stratify= y_train)\n",
    "kmx = X_train[np.array(y_train == 0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f4c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use K-means to select 693 non-fraudulent cases\n",
    "km = KMeans(n_clusters= 693, random_state= 42).fit(kmx)\n",
    "non_fraud_centers = km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b67f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ps = vstack([sparse.csr_matrix(non_fraud_centers), X_train[np.array(y_train == 1), :]])\n",
    "y_train_ps = np.concatenate([np.repeat(0, 693), np.repeat(1, 693)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be530172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "knn = KNeighborsClassifier(n_neighbors= 1)\n",
    "knn.fit(X_train_ps, y_train_ps)\n",
    "y_val_pred = knn.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18beb84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = knn.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred, digits= 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f2b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_val, y_val_pred)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf47b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = knn.predict(X_test)\n",
    "print(classification_report(y_test, y_test_pred, digits= 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133d1633",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_test_pred)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5bd0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2 = KNeighborsClassifier(n_neighbors= 1)\n",
    "knn2.fit(X_train, y_train)\n",
    "y_val_pred_2 = knn2.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d7a8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val, y_val_pred_2, digits= 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274807cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_val, y_val_pred_2)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d86f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_2 = knn2.predict(X_test)\n",
    "print(classification_report(y_test, y_test_pred_2, digits= 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c024ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_test_pred_2)\n",
    "metrics.auc(fpr, tpr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI-ML)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
